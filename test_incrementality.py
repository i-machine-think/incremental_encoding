"""
Test the incrementality of a model by computing different scores.
"""

# STD
import argparse
from collections import defaultdict

# EXT
from machine.util.checkpoint import Checkpoint
import torch
from machine.trainer import SupervisedTrainer
from machine.dataset import SourceField, TargetField
import torchtext
from machine.evaluator import Evaluator
import numpy as np
from scipy.stats import ttest_ind

# PROJECT
from incremental_metrics import AverageIntegrationRatio, DiagnosticClassifierAccuracy, \
    WeighedDiagnosticClassifierAccuracy, RepresentationalSimilarity

# GLOBALS
from incremental_models import IncrementalSeq2Seq

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
METRICS = {
    "integration_ratio": AverageIntegrationRatio,
    "dc_accuracy": DiagnosticClassifierAccuracy,
    "wdc_accuracy": WeighedDiagnosticClassifierAccuracy,
    "repr_sim": RepresentationalSimilarity
}

# CONSTANTS
TOP_N = 3


def main():
    parser = init_argparser()

    opt = parser.parse_args()

    if torch.cuda.is_available():
        print("Cuda device set to %i" % opt.cuda_device)
        torch.cuda.set_device(opt.cuda_device)

    parser = init_argparser()
    opt = parser.parse_args()

    # Prepare data set
    test, src, tgt = load_test_data(opt)

    # Load models
    models, input_vocab, output_vocab = load_models_from_paths(opt.models, src, tgt)
    pad = output_vocab.stoi[tgt.pad_token]

    metrics = [METRICS[metric](max_len=opt.max_len, pad=pad, n=TOP_N) for metric in opt.metrics]

    # Evaluate models on test set
    baseline_measurements = defaultdict(list)
    incremental_measurements = defaultdict(list)

    for model in models:
        evaluator = IncrementalEvaluator(metrics=metrics, batch_size=opt.batch_size)
        metrics = evaluator.evaluate(model, test, SupervisedTrainer.get_batch_data)

        print(type(model).__name__)
        print(
            "\n".join([f"{metric._NAME:<40}: {metric.get_val():4f}" for metric in metrics])
        )
        print("")

        for metric in metrics:
            if is_incremental(model):
                incremental_measurements[metric._SHORTNAME].append(metric.get_val())
            else:
                baseline_measurements[metric._SHORTNAME].append(metric.get_val())

    # Evaluate all the models together and generate final report
    print("\nResults per metric")
    for metric in baseline_measurements.keys():
        baseline_results, incremental_results = np.array(baseline_measurements[metric]), np.array(incremental_measurements[metric])
        baseline_avg, baseline_std = baseline_results.mean(), baseline_results.std()
        incremental_avg, incremental_std = incremental_results.mean(), baseline_results.std()
        _, p_value = ttest_ind(baseline_results, incremental_results)

        print(
            f"{metric:<10}: Baseline {baseline_avg:.4f} ±{baseline_std:.2f} | Incremental {incremental_avg:.4f} "
            f"±{incremental_std:.3f} | p={p_value:.4f}"
        )


class IncrementalEvaluator(Evaluator):
    def update_batch_metrics(self, metrics, other, target_variable):
        """
        Update a list with metrics for current batch.

        Args:
            metrics (list): list with of machine.metric.Metric objects
            other (dict): dict generated by forward pass of model to be evaluated
            target_variable (dict): map of keys to different targets of model

        Returns:
            metrics (list): list with updated metrics
        """
        # evaluate output symbols
        outputs = other['sequence']

        for metric in metrics:
            metric.eval_batch(outputs, other)

        return metrics

    def evaluate(self, model, data, get_batch_data):
        """ Evaluate a model on given dataset and return performance.

        Args:
            model (machine.models): model to evaluate
            data (machine.dataset.dataset.Dataset): dataset to evaluate against

        Returns:
            loss (float): loss of the given model on the given dataset
            accuracy (float): accuracy of the given model on the given dataset
        """
        # If the model was in train mode before this method was called, we make sure it still is
        # after this method.
        previous_train_mode = model.training
        model.eval()

        metrics = self.metrics
        for metric in metrics:
            metric.reset()

        # create batch iterator
        batch_iterator = torchtext.data.BucketIterator(
            dataset=data, batch_size=self.batch_size,
            sort=True, sort_key=lambda x: len(x.src),
            device=device, train=False
        )

        # loop over batches
        with torch.no_grad():
            for batch in batch_iterator:
                input_variable, input_lengths, target_variable = get_batch_data(
                    batch)

                decoder_outputs, decoder_hidden, other = model(
                    input_variable, input_lengths.tolist(), target_variable
                )

                # Get other necessary information for eval
                other["input_sequences"] = input_variable
                encoder_results = model.encoder_module(input_variable, input_lengths)
                other["encoder_hidden"] = encoder_results[0]
                other["encoder_embeddings"] = model.encoder_module.embedding(input_variable)

                # Compute metric(s) over one batch
                metrics = self.update_batch_metrics(
                    metrics, other, target_variable)

        model.train(previous_train_mode)

        return metrics


def load_test_data(opt):
    src = SourceField()
    tgt = TargetField()
    tabular_data_fields = [('src', src), ('tgt', tgt)]

    max_len = opt.max_len

    def len_filter(example):
        return len(example.src) <= max_len and len(example.tgt) <= max_len

    # generate training and testing data
    test = torchtext.data.TabularDataset(
        path=opt.test, format='tsv',
        fields=tabular_data_fields,
        filter_pred=len_filter
    )

    return test, src, tgt


def load_models_from_paths(paths: list, src, tgt):
    """
    Load all the models specified in a list of paths.
    """
    models = []

    for path in paths:
        checkpoint = Checkpoint.load(path)
        models.append(checkpoint.model)

    # Build vocab once
    input_vocab = checkpoint.input_vocab
    src.vocab = input_vocab
    input_vocab = checkpoint.input_vocab
    src.vocab = input_vocab
    output_vocab = checkpoint.output_vocab
    tgt.vocab = output_vocab
    tgt.eos_id = tgt.vocab.stoi[tgt.SYM_EOS]
    tgt.sos_id = tgt.vocab.stoi[tgt.SYM_SOS]

    return models, input_vocab, output_vocab


def is_incremental(model):
    """
    Test whether a model is of an incremental model class.
    """
    return isinstance(model, IncrementalSeq2Seq)


def init_argparser():
    parser = argparse.ArgumentParser()

    # Model arguments
    parser.add_argument('--test', help='Testing data')
    parser.add_argument('--metrics', nargs='+', default=['seq_acc'],
                        choices=["integration_ratio", "dc_accuracy", "wdc_accuracy", "repr_sim"], help='Metrics to use')
    parser.add_argument('--batch_size', type=int,
                        help='Batch size', default=1)
    # Data management
    parser.add_argument('--cuda_device', default=0,
                        type=int, help='set cuda device to use')
    parser.add_argument('--max_len', type=int,
                        help='Maximum sequence length', default=50)
    parser.add_argument('--output_dir', default='../models',
                        help='Path to model directory. If load_checkpoint is True, then path to checkpoint directory has to be provided')
    parser.add_argument("--models", nargs="+", help="List of paths to models used to conduct analyses.")

    return parser


if __name__ == "__main__":
    main()
