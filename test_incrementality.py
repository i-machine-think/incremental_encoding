"""
Test the incrementality of a model by computing different scores.
"""

# STD
import argparse

# EXT
from train_model import  load_model_from_checkpoint  # machine
import torch
from machine.trainer import SupervisedTrainer
from machine.dataset import SourceField, TargetField
import torchtext
from machine.evaluator import Evaluator

# PROJECT
from incremental_metrics import AverageIntegrationRatio

# GLOBALS
METRICS = {
    "integration_ratio": AverageIntegrationRatio
}


def main():
    parser = init_argparser()

    opt = parser.parse_args()

    if torch.cuda.is_available():
        print("Cuda device set to %i" % opt.cuda_device)
        torch.cuda.set_device(opt.cuda_device)

    parser = init_argparser()
    opt = parser.parse_args()

    # Prepare data set
    test, src, tgt = load_test_data(opt)

    # Prepare model
    seq2seq, input_vocab, output_vocab = load_model_from_checkpoint(opt, src, tgt)

    metrics = [METRICS[metric]() for metric in opt.metrics]

    #################################################################################
    # Evaluate model on test set

    evaluator = IncrementalEvaluator(metrics=metrics, batch_size=opt.batch_size)
    _, metrics = evaluator.evaluate(seq2seq, test, SupervisedTrainer.get_batch_data)

    print(["{}: {:6f}".format(type(metric).__name__, metric.get_val()) for metric in metrics])


class IncrementalEvaluator(Evaluator):
    def update_batch_metrics(self, metrics, other, target_variable):
        """
        Update a list with metrics for current batch.

        Args:
            metrics (list): list with of machine.metric.Metric objects
            other (dict): dict generated by forward pass of model to be evaluated
            target_variable (dict): map of keys to different targets of model

        Returns:
            metrics (list): list with updated metrics
        """
        # evaluate output symbols
        outputs = other['sequence']

        for metric in metrics:
            metric.eval_batch(outputs, other)

        return metrics


def load_test_data(opt):
    src = SourceField()
    tgt = TargetField()
    tabular_data_fields = [('src', src), ('tgt', tgt)]

    max_len = opt.max_len

    def len_filter(example):
        return len(example.src) <= max_len and len(example.tgt) <= max_len

    # generate training and testing data
    test = torchtext.data.TabularDataset(
        path=opt.test, format='tsv',
        fields=tabular_data_fields,
        filter_pred=len_filter
    )

    return test, src, tgt


def init_argparser():
    parser = argparse.ArgumentParser()

    # Model arguments
    parser.add_argument('--test', help='Testing data')
    parser.add_argument('--metrics', nargs='+', default=['seq_acc'], choices=["integration_ratio"],
                        help='Metrics to use')
    parser.add_argument('--batch_size', type=int,
                        help='Batch size', default=1)
    # Data management
    parser.add_argument('--load_checkpoint',
                        help='The name of the checkpoint to load, usually an encoded time string')
    parser.add_argument('--cuda_device', default=0,
                        type=int, help='set cuda device to use')
    parser.add_argument('--max_len', type=int,
                        help='Maximum sequence length', default=50)
    parser.add_argument('--output_dir', default='../models',
                        help='Path to model directory. If load_checkpoint is True, then path to checkpoint directory has to be provided')

    return parser


if __name__ == "__main__":
    main()
