"""
Test the incrementality of a model by computing different scores.
"""

# STD
import argparse

# EXT
from train_model import  load_model_from_checkpoint  # machine
import torch
from machine.trainer import SupervisedTrainer
from machine.dataset import SourceField, TargetField
import torchtext
from machine.evaluator import Evaluator

# PROJECT
from incremental_metrics import AverageIntegrationRatio

# GLOBALS
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
METRICS = {
    "integration_ratio": AverageIntegrationRatio
}


def main():
    parser = init_argparser()

    opt = parser.parse_args()

    if torch.cuda.is_available():
        print("Cuda device set to %i" % opt.cuda_device)
        torch.cuda.set_device(opt.cuda_device)

    parser = init_argparser()
    opt = parser.parse_args()

    # Prepare data set
    test, src, tgt = load_test_data(opt)

    # Prepare model
    seq2seq, input_vocab, output_vocab = load_model_from_checkpoint(opt, src, tgt)

    metrics = [METRICS[metric]() for metric in opt.metrics]

    #################################################################################
    # Evaluate model on test set

    evaluator = IncrementalEvaluator(metrics=metrics, batch_size=opt.batch_size)
    metrics = evaluator.evaluate(seq2seq, test, SupervisedTrainer.get_batch_data)

    print(["{}: {:6f}".format(type(metric).__name__, metric.get_val()) for metric in metrics])


class IncrementalEvaluator(Evaluator):
    def update_batch_metrics(self, metrics, other, target_variable):
        """
        Update a list with metrics for current batch.

        Args:
            metrics (list): list with of machine.metric.Metric objects
            other (dict): dict generated by forward pass of model to be evaluated
            target_variable (dict): map of keys to different targets of model

        Returns:
            metrics (list): list with updated metrics
        """
        # evaluate output symbols
        outputs = other['sequence']

        for metric in metrics:
            metric.eval_batch(outputs, other)

        return metrics

    def evaluate(self, model, data, get_batch_data):
        """ Evaluate a model on given dataset and return performance.

        Args:
            model (machine.models): model to evaluate
            data (machine.dataset.dataset.Dataset): dataset to evaluate against

        Returns:
            loss (float): loss of the given model on the given dataset
            accuracy (float): accuracy of the given model on the given dataset
        """
        # If the model was in train mode before this method was called, we make sure it still is
        # after this method.
        previous_train_mode = model.training
        model.eval()

        metrics = self.metrics
        for metric in metrics:
            metric.reset()

        # create batch iterator
        batch_iterator = torchtext.data.BucketIterator(
            dataset=data, batch_size=self.batch_size,
            sort=True, sort_key=lambda x: len(x.src),
            device=device, train=False)

        # loop over batches
        with torch.no_grad():
            for batch in batch_iterator:
                input_variable, input_lengths, target_variable = get_batch_data(
                    batch)

                decoder_outputs, decoder_hidden, other = model(
                    input_variable, input_lengths.tolist(), target_variable
                )

                # Get other necessary information for eval
                encoder_results = model.encoder_module(input_variable, input_lengths)
                other["encoder_hidden"] = encoder_results[1]
                other["encoder_embeddings"] = model.encoder_module.embedding(input_variable)

                # Compute metric(s) over one batch
                metrics = self.update_batch_metrics(
                    metrics, other, target_variable)

        model.train(previous_train_mode)

        return metrics


def load_test_data(opt):
    src = SourceField()
    tgt = TargetField()
    tabular_data_fields = [('src', src), ('tgt', tgt)]

    max_len = opt.max_len

    def len_filter(example):
        return len(example.src) <= max_len and len(example.tgt) <= max_len

    # generate training and testing data
    test = torchtext.data.TabularDataset(
        path=opt.test, format='tsv',
        fields=tabular_data_fields,
        filter_pred=len_filter
    )

    return test, src, tgt


def init_argparser():
    parser = argparse.ArgumentParser()

    # Model arguments
    parser.add_argument('--test', help='Testing data')
    parser.add_argument('--metrics', nargs='+', default=['seq_acc'], choices=["integration_ratio"],
                        help='Metrics to use')
    parser.add_argument('--batch_size', type=int,
                        help='Batch size', default=1)
    # Data management
    parser.add_argument('--load_checkpoint',
                        help='The name of the checkpoint to load, usually an encoded time string')
    parser.add_argument('--cuda_device', default=0,
                        type=int, help='set cuda device to use')
    parser.add_argument('--max_len', type=int,
                        help='Maximum sequence length', default=50)
    parser.add_argument('--output_dir', default='../models',
                        help='Path to model directory. If load_checkpoint is True, then path to checkpoint directory has to be provided')

    return parser


if __name__ == "__main__":
    main()
